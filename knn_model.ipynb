{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "class SpotifyRecommender:\n",
    "    supported_values = ['acousticness', 'danceability', 'duration_ms', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'speechiness', 'tempo', 'time_signature', 'valence'] + [f'Chroma_{i}' for i in range(1, 13)] + [f'MEL_{i}' for i in range(1, 129)] + [f'MFCC_{i}' for i in range(1, 49)] + [f'Spectral_contrast_{i}' for i in range(1, 8)] + [f'Tonnetz_{i}' for i in range(1, 7)] + ['ZCR', 'entropy_energy', 'spectral_bandwith', 'spectral_centroid', 'spectral_rollOff_max', 'spectral_rollOff_min']\n",
    "    supported_categories = ['artists_genres']\n",
    "    evaluation_result = []\n",
    "    def __init__(self,k=20):\n",
    "        self.scaler = None\n",
    "        self.pca = None\n",
    "        self.sfm = None\n",
    "        self.knn = None\n",
    "        self.k = k\n",
    "        self.filepath = None\n",
    "        self.dataset = pd.DataFrame()\n",
    "        self.features = None\n",
    "        self.best_features = ['acousticness','danceability','duration_ms','energy','instrumentalness'\n",
    "                            'liveness','loudness','speechiness','tempo','valence','Chroma_1'\n",
    "                            'Chroma_2','Chroma_3','Chroma_4','Chroma_5','Chroma_6','Chroma_7'\n",
    "                            'Chroma_8','Chroma_9','Chroma_10','Chroma_11','Chroma_12','MEL_1','MEL_2'\n",
    "                            'MEL_3','MEL_4','MEL_5','MEL_6','MEL_7','MEL_8','MEL_13','MEL_14'\n",
    "                            'MEL_16','MEL_17','MEL_18','MEL_19','MEL_20','MEL_22','MEL_23','MEL_24'\n",
    "                            'MEL_27','MEL_30','MEL_51','MFCC_2','MFCC_3','MFCC_4','MFCC_5','MFCC_6'\n",
    "                            'MFCC_7','MFCC_8','MFCC_9','MFCC_10','MFCC_11','MFCC_12','MFCC_13'\n",
    "                            'MFCC_14','MFCC_15','MFCC_16','MFCC_17','MFCC_18','MFCC_19','MFCC_20'\n",
    "                            'MFCC_21','MFCC_22','MFCC_23','MFCC_24','MFCC_25','MFCC_26','MFCC_27'\n",
    "                            'MFCC_28','MFCC_29','MFCC_30','MFCC_31','MFCC_32','MFCC_33','MFCC_34'\n",
    "                            'MFCC_35','MFCC_36','MFCC_37','MFCC_38','MFCC_39','MFCC_40','MFCC_41'\n",
    "                            'MFCC_42','MFCC_43','MFCC_44','MFCC_45','MFCC_46','MFCC_47','MFCC_48'\n",
    "                            'Spectral_contrast_1','Spectral_contrast_2','Spectral_contrast_3'\n",
    "                            'Spectral_contrast_4','Spectral_contrast_5','Spectral_contrast_6'\n",
    "                            'Spectral_contrast_7','Tonnetz_1','Tonnetz_2','Tonnetz_3','Tonnetz_4'\n",
    "                            'Tonnetz_5','Tonnetz_6','entropy_energy','spectral_centroid'\n",
    "                            'spectral_rollOff_min']\n",
    "        self.reduced_data = None\n",
    "        self.encoded_data = None\n",
    "        self.feature_df = None\n",
    "        self.trained_values = []\n",
    "        self.trained_categories = []\n",
    "        \n",
    "    def __get_features_in_dataset(self):\n",
    "        listed = self.dataset.columns.tolist()\n",
    "        new_featurelist = []\n",
    "        for f in listed:\n",
    "            if f in self.supported_values:\n",
    "                new_featurelist.append(f)\n",
    "        \n",
    "        self.features = new_featurelist\n",
    "\n",
    "    def import_dataset(self,df):\n",
    "        self.dataset = df\n",
    "        self.__get_features_in_dataset()\n",
    "\n",
    "\n",
    "    def evaluate_features_in_dataset(self):\n",
    "        '''\n",
    "        Duration on a Dell G5 Notebook\n",
    "        Scale: 0.3 sec\n",
    "        RandomForestRegressor: 71 min\n",
    "        SelectFromModel: 72 min\n",
    "        PCA: 0.8 sec\n",
    "        '''\n",
    "        self.scaler = StandardScaler()\n",
    "        scaled_data = self.scaler.fit_transform(self.dataset[self.features])\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(scaled_data, self.dataset.index) \n",
    "\n",
    "        self.sfm = SelectFromModel(rf, threshold='mean')\n",
    "        self.sfm.fit(scaled_data, self.dataset.index) \n",
    "\n",
    "        self.pca = PCA(n_components=0.95)\n",
    "        self.reduced_data = self.pca.fit_transform(scaled_data[:, self.sfm.get_support()])\n",
    "\n",
    "        self.best_features = self.dataset[self.features].columns[self.sfm.get_support()].tolist()\n",
    "\n",
    "        \n",
    "    def binarize_categories(self,dataset,categories):\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        encoded = mlb.fit_transform(dataset[categories])\n",
    "        return pd.DataFrame(encoded, columns=mlb.classes_)\n",
    "\n",
    "    def create_model(self):\n",
    "        '''KNN: 18.5 sec'''\n",
    "        print(f'---- Creating model ----')\n",
    "        self.knn = NearestNeighbors()\n",
    "        self.trained_values = self.best_features\n",
    "\n",
    "        if len(self.trained_categories) > 0:\n",
    "            self.trained_categories = self.supported_categories\n",
    "            encoded_df = self.binarize_categories(self.dataset,self.trained_categories)\n",
    "            reduced_df = pd.DataFrame(self.reduced_data, columns=[f\"PC{i+1}\" for i in range(self.reduced_data.shape[1])])\n",
    "            self.feature_df = reduced_df.join(encoded_df)\n",
    "            self.knn.fit(self.feature_df)\n",
    "            \n",
    "        else:\n",
    "            self.knn.fit(self.reduced_data)\n",
    "            \n",
    "\n",
    "    def get_neighbors(self, predict_df, k=None):\n",
    "        if k == None:\n",
    "            k = self.k\n",
    "        self.knn.n_neighbors = k\n",
    "\n",
    "        value_df= predict_df[self.trained_values]\n",
    "        new_scaled_data = self.scaler.transform(value_df)\n",
    "        new_reduced_data = self.pca.transform(new_scaled_data[:, self.sfm.get_support()])\n",
    "\n",
    "        if len(self.trained_categories) > 0:\n",
    "            reduced_df = pd.DataFrame(new_reduced_data, columns=[f\"PC{i+1}\" for i in range(new_reduced_data.shape[1])])\n",
    "            category_df = predict_df[self.trained_categories]\n",
    "            encoded_df = self.binarize_categories(predict_df,self.trained_categories)\n",
    "\n",
    "            to_predict = reduced_df.join(encoded_df)\n",
    "        else:\n",
    "            to_predict = new_reduced_data\n",
    "\n",
    "        distances, indices = self.knn.kneighbors(to_predict)\n",
    "        \n",
    "        return self.dataset.iloc[indices[0]]\n",
    "\n",
    "    def save(self, file_path=None):\n",
    "        if file_path == None:\n",
    "            file_path = self.filepath\n",
    "        model_data = {\n",
    "            'knn': self.knn,\n",
    "            'scaler': self.scaler,\n",
    "            'pca': self.pca,\n",
    "            'sfm': self.sfm,\n",
    "            'filepath': file_path,\n",
    "            'trained_values':self.trained_values,\n",
    "            'trained_categories':self.trained_categories\n",
    "        }\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        self.knn = model_data['knn']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.pca = model_data['pca']\n",
    "        self.sfm = model_data['sfm']\n",
    "        self.filepath = model_data['filepath']\n",
    "        self.trained_values = model_data['trained_values']\n",
    "        self.trained_categories = model_data['trained_categories']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\Analyseanwendungen\\lib\\site-packages\\dagster\\_core\\definitions\\composition.py:184: UserWarning: While in @job context 'prepare_infos', received an uninvoked op 'save_pickl'.\n",
      "  warnings.warn(warning_message.strip())\n"
     ]
    },
    {
     "ename": "DagsterInvalidDefinitionError",
     "evalue": "In @job prepare_infos, received invalid type <class 'str'> for input \"name\" (at position 1) in op invocation \"save_pickl\". Must pass the output from previous node invocations or inputs to the composition function as inputs when invoking nodes during composition.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDagsterInvalidDefinitionError\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39metl\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39metl\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[39m=\u001b[39m etl\u001b[39m.\u001b[39mprepare_dataset()\n",
      "File \u001b[1;32mc:\\Users\\stefa\\OneDrive - FHWN\\Privat\\Studium\\MIT_2-Semester\\Analyseanwendungen\\spotify_recommender\\etl.py:111\u001b[0m\n\u001b[0;32m    107\u001b[0m     data \u001b[39m=\u001b[39m match_spotify_data(tracks,albums,artists,audio_features,lyrics_features)\n\u001b[0;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[0;32m    110\u001b[0m \u001b[39m@job\u001b[39;49m\n\u001b[1;32m--> 111\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mprepare_infos\u001b[39;49m(df \u001b[39m=\u001b[39;49m prepare_dataset()):\n\u001b[0;32m    112\u001b[0m     track_infos \u001b[39m=\u001b[39;49m track_info(df)\n\u001b[0;32m    114\u001b[0m     save_pickl(track_infos,\u001b[39m'\u001b[39;49m\u001b[39mtrack_info\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Analyseanwendungen\\lib\\site-packages\\dagster\\_core\\definitions\\decorators\\job_decorator.py:237\u001b[0m, in \u001b[0;36mjob\u001b[1;34m(compose_fn, name, description, resource_defs, config, tags, metadata, logger_defs, executor_def, hooks, op_retry_policy, version_strategy, partitions_def, input_values)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m compose_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     check\u001b[39m.\u001b[39minvariant(description \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 237\u001b[0m     \u001b[39mreturn\u001b[39;00m _Job()(compose_fn)\n\u001b[0;32m    239\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdagster\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecution\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuild_resources\u001b[39;00m \u001b[39mimport\u001b[39;00m wrap_resources_for_execution\n\u001b[0;32m    241\u001b[0m \u001b[39mreturn\u001b[39;00m _Job(\n\u001b[0;32m    242\u001b[0m     name\u001b[39m=\u001b[39mname,\n\u001b[0;32m    243\u001b[0m     description\u001b[39m=\u001b[39mdescription,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m     input_values\u001b[39m=\u001b[39minput_values,\n\u001b[0;32m    255\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Analyseanwendungen\\lib\\site-packages\\dagster\\_core\\definitions\\decorators\\job_decorator.py:74\u001b[0m, in \u001b[0;36m_Job.__call__\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdagster\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_core\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdefinitions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m do_composition\n\u001b[0;32m     66\u001b[0m (\n\u001b[0;32m     67\u001b[0m     input_mappings,\n\u001b[0;32m     68\u001b[0m     output_mappings,\n\u001b[0;32m     69\u001b[0m     dependencies,\n\u001b[0;32m     70\u001b[0m     solid_defs,\n\u001b[0;32m     71\u001b[0m     config_mapping,\n\u001b[0;32m     72\u001b[0m     positional_inputs,\n\u001b[0;32m     73\u001b[0m     node_input_source_assets,\n\u001b[1;32m---> 74\u001b[0m ) \u001b[39m=\u001b[39m do_composition(\n\u001b[0;32m     75\u001b[0m     decorator_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m@job\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     76\u001b[0m     graph_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m     77\u001b[0m     fn\u001b[39m=\u001b[39;49mfn,\n\u001b[0;32m     78\u001b[0m     provided_input_defs\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m     79\u001b[0m     provided_output_defs\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m     80\u001b[0m     ignore_output_from_composition_fn\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     81\u001b[0m     config_mapping\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     84\u001b[0m graph_def \u001b[39m=\u001b[39m GraphDefinition(\n\u001b[0;32m     85\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[0;32m     86\u001b[0m     dependencies\u001b[39m=\u001b[39mdependencies,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m     node_input_source_assets\u001b[39m=\u001b[39mnode_input_source_assets,\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     97\u001b[0m job_def \u001b[39m=\u001b[39m graph_def\u001b[39m.\u001b[39mto_job(\n\u001b[0;32m     98\u001b[0m     description\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdescription \u001b[39mor\u001b[39;00m format_docstring_for_description(fn),\n\u001b[0;32m     99\u001b[0m     resource_defs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresource_defs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m     input_values\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_values,\n\u001b[0;32m    110\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Analyseanwendungen\\lib\\site-packages\\dagster\\_core\\definitions\\composition.py:1064\u001b[0m, in \u001b[0;36mdo_composition\u001b[1;34m(decorator_name, graph_name, fn, provided_input_defs, provided_output_defs, config_mapping, ignore_output_from_composition_fn)\u001b[0m\n\u001b[0;32m   1062\u001b[0m enter_composition(graph_name, decorator_name)\n\u001b[0;32m   1063\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1064\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m ignore_output_from_composition_fn:\n\u001b[0;32m   1066\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\stefa\\OneDrive - FHWN\\Privat\\Studium\\MIT_2-Semester\\Analyseanwendungen\\spotify_recommender\\etl.py:114\u001b[0m, in \u001b[0;36mprepare_infos\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39m@job\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_infos\u001b[39m(df \u001b[39m=\u001b[39m prepare_dataset()):\n\u001b[0;32m    112\u001b[0m     track_infos \u001b[39m=\u001b[39m track_info(df)\n\u001b[1;32m--> 114\u001b[0m     save_pickl(track_infos,\u001b[39m'\u001b[39;49m\u001b[39mtrack_info\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Analyseanwendungen\\lib\\site-packages\\dagster\\_core\\definitions\\op_definition.py:391\u001b[0m, in \u001b[0;36mOpDefinition.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdecorators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mop_decorator\u001b[39;00m \u001b[39mimport\u001b[39;00m DecoratedOpFunction\n\u001b[0;32m    390\u001b[0m \u001b[39mif\u001b[39;00m is_in_composition():\n\u001b[1;32m--> 391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(OpDefinition, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m     node_label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_type_str  \u001b[39m# string \"solid\" for solids, \"op\" for ops\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Analyseanwendungen\\lib\\site-packages\\dagster\\_core\\definitions\\node_definition.py:222\u001b[0m, in \u001b[0;36mNodeDefinition.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: \u001b[39mobject\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: \u001b[39mobject\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_pending_invocation()(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Analyseanwendungen\\lib\\site-packages\\dagster\\_core\\definitions\\composition.py:504\u001b[0m, in \u001b[0;36mPendingNodeInvocation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[39mif\u001b[39;00m input_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m         \u001b[39mraise\u001b[39;00m DagsterInvalidDefinitionError(\n\u001b[0;32m    493\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIn \u001b[39m\u001b[39m{source}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m, could not resolve input based on position at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    494\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m{idx}\u001b[39;00m\u001b[39m for invocation \u001b[39m\u001b[39m{node_name}\u001b[39;00m\u001b[39m. Use keyword args instead, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m             )\n\u001b[0;32m    502\u001b[0m         )\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_argument_node(\n\u001b[0;32m    505\u001b[0m         node_name,\n\u001b[0;32m    506\u001b[0m         output_node,\n\u001b[0;32m    507\u001b[0m         input_name,\n\u001b[0;32m    508\u001b[0m         input_bindings,\n\u001b[0;32m    509\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m(at position \u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m)\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    510\u001b[0m     )\n\u001b[0;32m    512\u001b[0m \u001b[39m# then **kwargs\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[39mfor\u001b[39;00m input_name, output_node \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\Analyseanwendungen\\lib\\site-packages\\dagster\\_core\\definitions\\composition.py:649\u001b[0m, in \u001b[0;36mPendingNodeInvocation._process_argument_node\u001b[1;34m(self, node_name, output_node, input_name, input_bindings, arg_desc)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[39mraise\u001b[39;00m DagsterInvalidDefinitionError(\n\u001b[0;32m    635\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn \u001b[39m\u001b[39m{source}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m, received an un-invoked \u001b[39m\u001b[39m{described_node}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    636\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m for input \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         )\n\u001b[0;32m    647\u001b[0m     )\n\u001b[0;32m    648\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 649\u001b[0m     \u001b[39mraise\u001b[39;00m DagsterInvalidDefinitionError(\n\u001b[0;32m    650\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn \u001b[39m\u001b[39m{source}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m, received invalid type \u001b[39m\u001b[39m{type}\u001b[39;00m\u001b[39m for input \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{input_name}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{arg_desc}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{node_type}\u001b[39;00m\u001b[39m invocation \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{node_name}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    652\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMust pass the output from previous node invocations or inputs to the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    653\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcomposition function as inputs when invoking nodes during composition.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    654\u001b[0m             source\u001b[39m=\u001b[39mcurrent_context()\u001b[39m.\u001b[39msource,\n\u001b[0;32m    655\u001b[0m             name\u001b[39m=\u001b[39mcurrent_context()\u001b[39m.\u001b[39mname,\n\u001b[0;32m    656\u001b[0m             \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(output_node),\n\u001b[0;32m    657\u001b[0m             arg_desc\u001b[39m=\u001b[39marg_desc,\n\u001b[0;32m    658\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[0;32m    659\u001b[0m             node_name\u001b[39m=\u001b[39mnode_name,\n\u001b[0;32m    660\u001b[0m             node_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_def\u001b[39m.\u001b[39mnode_type_str,\n\u001b[0;32m    661\u001b[0m         )\n\u001b[0;32m    662\u001b[0m     )\n",
      "\u001b[1;31mDagsterInvalidDefinitionError\u001b[0m: In @job prepare_infos, received invalid type <class 'str'> for input \"name\" (at position 1) in op invocation \"save_pickl\". Must pass the output from previous node invocations or inputs to the composition function as inputs when invoking nodes during composition."
     ]
    }
   ],
   "source": [
    "import etl as etl\n",
    "df = etl.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'evaluation/dataset.pickle'\n",
    "df.to_pickle(dataset_path)\n",
    "# df = pd.read_pickle(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r'evaluation/SpotifyRecommenderV1.pickle'\n",
    "\n",
    "# sp = SpotifyRecommender().import_dataset(df) # dataset wird nur für das training benötigt\n",
    "sp = SpotifyRecommender()\n",
    "sp.load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
